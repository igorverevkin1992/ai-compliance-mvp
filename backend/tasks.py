import os
import json
import re
import subprocess
import asyncio
import time
import random
import google.generativeai as genai
from sqlalchemy import text

from celery_app import app
from prompts.instructions import SYSTEM_PROMPT_TEMPLATE
from shazam_helper import recognize_music
from database import SessionLocal, init_db

# --- –ù–ê–°–¢–†–û–ô–ö–ò ---
SAFETY_SETTINGS = [
    {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
]

def get_embedding(text_to_embed: str, api_key: str):
    
    try:
        genai.configure(api_key=api_key)
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—É—é –ª–µ–≥–∫—É—é –º–æ–¥–µ–ª—å –¥–ª—è –≤–µ–∫—Ç–æ—Ä–æ–≤
        result = genai.embed_content(
            model="models/text-embedding-004",
            content=text_to_embed,
            task_type="retrieval_document"
        )
        return result['embedding']
    except Exception as e:
        print(f"‚ö†Ô∏è Embedding Error: {e}")
        return None

def clean_json_text(text: str) -> str:
    text = re.sub(r"^```json\s*", "", text, flags=re.MULTILINE)
    text = re.sub(r"^```\s*", "", text, flags=re.MULTILINE)
    text = re.sub(r"```$", "", text, flags=re.MULTILINE)
    return text.strip()

def compress_media(input_path: str) -> tuple[str, str]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (–ø—É—Ç—å_–∫_—Å–∂–∞—Ç–æ–º—É_—Ñ–∞–π–ª—É, mime_type)
    –ï—Å–ª–∏ —ç—Ç–æ –≤–∏–¥–µ–æ - —Å–∂–∏–º–∞–µ—Ç —Ä–∞–∑–º–µ—Ä –∫–∞–¥—Ä–∞, –Ω–æ –û–°–¢–ê–í–õ–Ø–ï–¢ –í–ò–î–ï–û.
    –ï—Å–ª–∏ –∞—É–¥–∏–æ - –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –≤ –ª–µ–≥–∫–∏–π AAC.
    """
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –≤–∏–¥–µ–æ —ç—Ç–æ –∏–ª–∏ –∞—É–¥–∏–æ, —Å –ø–æ–º–æ—â—å—é ffprobe (–∏–ª–∏ –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é)
    ext = input_path.split('.')[-1].lower()
    is_video = ext in ['mp4', 'mov', 'avi', 'mkv', 'webm']
    
    output_filename = f"{os.path.splitext(input_path)[0]}_compressed"
    
    if is_video:
        # –°–∂–∏–º–∞–µ–º –í–ò–î–ï–û:
        # -vf scale=640:-2 : –£–º–µ–Ω—å—à–∞–µ–º —à–∏—Ä–∏–Ω—É –¥–æ 640px (–≤—ã—Å–æ—Ç–∞ –∞–≤—Ç–æ), —á—Ç–æ–±—ã Gemini –≤–∏–¥–µ–ª –∫–∞—Ä—Ç–∏–Ω–∫—É, –Ω–æ —Ñ–∞–π–ª –±—ã–ª –ª–µ–≥–∫–∏–º
        # -crf 28 : –°—Ä–µ–¥–Ω–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ (—á–µ–º –≤—ã—à–µ —á–∏—Å–ª–æ, —Ç–µ–º —Ö—É–∂–µ –∫–∞—á–µ—Å—Ç–≤–æ –∏ –º–µ–Ω—å—à–µ –≤–µ—Å)
        # -r 24 : 24 –∫–∞–¥—Ä–∞ –≤ —Å–µ–∫—É–Ω–¥—É
        output_path = f"{output_filename}.mp4"
        command = [
            "ffmpeg", "-y", "-i", input_path,
            "-vf", "scale=640:-2,format=yuv420p", # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –ø–∏–∫—Å–µ–ª–µ–π yuv420p
            "-c:v", "libx264", 
            "-profile:v", "high", # –ü—Ä–æ—Ñ–∏–ª—å —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
            "-level", "4.1",
            "-crf", "28", 
            "-preset", "faster", 
            "-r", "24",
            "-c:a", "aac", "-ac", "1", "-ar", "16000",
            output_path
        ]
        mime = "video/mp4"
    else:
        # –°–∂–∏–º–∞–µ–º –ê–£–î–ò–û (–∫–∞–∫ —Ä–∞–Ω—å—à–µ):
        output_path = f"{output_filename}.m4a"
        command = [
            "ffmpeg", "-y", "-i", input_path,
            "-vn", "-ac", "1", "-ar", "16000", "-c:a", "aac", 
            output_path
        ]
        mime = "audio/mp4"

    try:
        print(f"üé¨ Starting Compression ({mime}) for {input_path}...")
        subprocess.run(command, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        if os.path.exists(output_path):
            size_mb = os.path.getsize(output_path) / (1024 * 1024)
            print(f"‚úÖ Compression success: {output_path} ({size_mb:.2f} MB)")
            return output_path, mime
    except Exception as e:
        print(f"‚ö†Ô∏è FFmpeg Error: {e}")
        # –ï—Å–ª–∏ —Å–∂–∞—Ç–∏–µ –Ω–µ –≤—ã—à–ª–æ, –≤–µ—Ä–Ω–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª
        return input_path, ("video/mp4" if is_video else "audio/mp3")
    
    return input_path, "application/octet-stream"

def upload_to_gemini(path: str, mime_type: str):
    """–ó–∞–≥—Ä—É–∑–∫–∞ —Å –º–≥–Ω–æ–≤–µ–Ω–Ω—ã–º –æ—Ç–ª–æ–≤–æ–º –æ—à–∏–±–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
    print(f"‚òÅÔ∏è Uploading to Gemini: {path}")
    try:
        file = genai.upload_file(path, mime_type=mime_type)
        
        # –ñ–¥–µ–º –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏
        start_time = time.time()
        while file.state.name == "PROCESSING":
            time.sleep(3)
            file = genai.get_file(file.name)
            # –ï—Å–ª–∏ –º–∞–ª–µ–Ω—å–∫–∏–π —Ñ–∞–π–ª –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –¥–æ–ª—å—à–µ 40 —Å–µ–∫—É–Ω–¥ - —ç—Ç–æ —É–∂–µ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ
            if time.time() - start_time > 300:
                print("‚ùå Google –∑–∞—Å—Ç—Ä—è–ª –Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–µ (–≤–æ–∑–º–æ–∂–Ω–æ, –Ω–µ—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π –∫–æ–¥–µ–∫).")
                return None

        if file.state.name == "FAILED":
            print(f"‚ùå Google –Ω–µ —Å–º–æ–≥ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ñ–∞–π–ª. –°–æ—Å—Ç–æ—è–Ω–∏–µ: {file.state.name}")
            return None

        print(f"‚úÖ –§–∞–π–ª ACTIVE –∑–∞ {int(time.time() - start_time)} —Å–µ–∫.")
        return file
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ API Google –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ: {e}")
        return None

def get_rag_context(db, profile, query_text, api_key):
    """–ò—â–µ—Ç –≤ –±–∞–∑–µ 5 —Å–∞–º—ã—Ö –ø–æ—Ö–æ–∂–∏—Ö –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö –∫–µ–π—Å–æ–≤ —á–µ—Ä–µ–∑ –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫"""
    try:
        # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –ü–æ–ª–∏—Ç–∏–∫–∏ (–∫–∞–∫ –∏ —Ä–∞–Ω—å—à–µ)
        pub_query = "YouTube%" if profile == "youtube" else "–ù–¢–í%"
        sql_pol = text("""
            SELECT r.req_code, r.summary, r.full_text 
            FROM legal_requirement r
            JOIN legal_doc d ON r.doc_id = d.id
            WHERE d.publisher LIKE :pub
        """)
        policies = db.execute(sql_pol, {"pub": pub_query}).fetchall()
        policies_text = "\n".join([f"- [{p.req_code}] {p.summary}" for p in policies])

        # 2. –í–ï–ö–¢–û–†–ù–´–ô –ü–û–ò–°–ö –ü–û –ü–ê–ú–Ø–¢–ò (Semantic RAG)
        vector = get_embedding(query_text, api_key)
        human_examples = "–ü–æ—Ö–æ–∂–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ."
        
        if vector:
            # –ò—â–µ–º –≤ —Ç–∞–±–ª–∏—Ü–µ case_memory —á–µ—Ä–µ–∑ –æ–ø–µ—Ä–∞—Ç–æ—Ä <=> (–∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ)
            sql_vector = text("""
                SELECT text, meta->>'final_risk' as risk
                FROM case_memory
                ORDER BY embedding <=> :vec_str
                LIMIT 5
            """)
            # –ü—Ä–µ–≤—Ä–∞—â–∞–µ–º —Å–ø–∏—Å–æ–∫ —á–∏—Å–µ–ª –≤ —Å—Ç—Ä–æ–∫—É, –∫–æ—Ç–æ—Ä—É—é –ø–æ–π–º–µ—Ç Postgres
            similar_cases = db.execute(sql_vector, {"vec_str": str(vector)}).fetchall()
            
            if similar_cases:
                examples_list = [f"–ö–ï–ô–°: {c.text} | –í–ï–†–î–ò–ö–¢: {c.risk}" for c in similar_cases]
                human_examples = "\n\n".join(examples_list)

        return policies_text, human_examples
    except Exception as e:
        print(f"‚ö†Ô∏è RAG Error: {e}")
        return "–û—à–∏–±–∫–∞ –ø–æ–ª–∏—Ç–∏–∫", "–û—à–∏–±–∫–∞ –ø–∞–º—è—Ç–∏"

def save_results_to_db(db, asset_id, result_json, model_name):
    try:
        risk = result_json.get('overall', {}).get('risk_level', 'UNKNOWN')
        conf = result_json.get('overall', {}).get('confidence', 0.0)
        
        run_res = db.execute(text("""
            INSERT INTO agent_run (asset_id, model, output_json, overall_risk, overall_confidence)
            VALUES (:aid, :model_name, :json, :risk, :conf)
            RETURNING id
        """), {
            "aid": asset_id,
            "model_name": model_name,  # <--- –í–û–¢ –≠–¢–û–ì–û –ù–ï –•–í–ê–¢–ê–õ–û
            "json": json.dumps(result_json),
            "risk": risk,
            "conf": conf
        }).fetchone()
        run_id = run_res.id

        evidence_map = {}
        for ev in result_json.get('evidence', []):
            pay = {"text": ev.get('text_quote'), "notes": ev.get('notes')}
            ev_res = db.execute(text("""
                INSERT INTO evidence (asset_id, type, start_ms, end_ms, payload)
                VALUES (:aid, :type, :start, :end, :pay)
                RETURNING id
            """), {
                "aid": asset_id, "type": ev.get('type'), 
                "start": ev.get('start_ms', 0), "end": ev.get('end_ms', 0), 
                "pay": json.dumps(pay)
            }).fetchone()
            evidence_map[ev.get('id')] = ev_res.id

        for lbl in result_json.get('labels', []):
            db_ev_ids = [evidence_map[eid] for eid in lbl.get('evidence_ids', []) if eid in evidence_map]
            db.execute(text("""
                INSERT INTO label_detection (run_id, label_code, severity, confidence, rationale, evidence_ids)
                VALUES (:rid, :code, :sev, :conf, :rat, :evs)
            """), {
                "rid": run_id, "code": lbl.get('code'), "sev": lbl.get('severity'),
                "conf": lbl.get('confidence'), "rat": lbl.get('rationale'),
                "evs": db_ev_ids
            })

        for rec in result_json.get('recommendations', []):
            db.execute(text("""
                INSERT INTO recommendation (run_id, action, priority, params)
                VALUES (:rid, :act, :prio, :par)
            """), {
                "rid": run_id, "act": rec.get('action'), "prio": rec.get('priority'),
                "par": json.dumps(rec.get('params'))
            })
            
        db.commit()
        return run_id
    except Exception as e:
        print(f"‚ö†Ô∏è DB Save Error: {e}")
        return None
    
    

# --- MAIN TASK ---

@app.task(bind=True)
def analyze_media_task(self, file_path: str, filename: str, api_key: str, model_name: str, profile: str = "ntv"):
    # ^^^ –î–û–ë–ê–í–ò–õ model_name –í –ê–†–ì–£–ú–ï–ù–¢–´ ^^^
    
    files_cleanup = []
    compressed_path = None
    
    try:
        genai.configure(api_key=api_key)
        
        # –ò–°–ü–û–õ–¨–ó–£–ï–ú –í–´–ë–†–ê–ù–ù–£–Æ –ú–û–î–ï–õ–¨
        print(f"ü§ñ Using Model: {model_name}")
        MODEL_NAME = model_name 

        # 1. –û–ë–†–ê–ë–û–¢–ö–ê (–¢–ï–ü–ï–†–¨ –° –í–ò–î–ï–û!)
        self.update_state(state='PROGRESS', meta={'status': '–°–∂–∞—Ç–∏–µ –≤–∏–¥–µ–æ/–∞—É–¥–∏–æ...'})
        # –§—É–Ω–∫—Ü–∏—è —Ç–µ–ø–µ—Ä—å –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Ç—å –ò mime-type
        compressed_path, mime_type = compress_media(file_path)
        
        target_file = compressed_path if compressed_path else file_path

        # 2. Shazam
        shazam_text = ""
        try:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            res = loop.run_until_complete(recognize_music(target_file))
            loop.close()
            if res: shazam_text = f"SHAZAM IDENTIFICATION: {res}"
        except: pass

        # 3. –ó–∞–≥—Ä—É–∑–∫–∞ (–° –ø–æ–¥—Ä–æ–±–Ω—ã–º –¥–µ–±–∞–≥–æ–º)
        self.update_state(state='PROGRESS', meta={'status': '–û—Ç–ø—Ä–∞–≤–∫–∞ –≤ Google Cloud...'})
        media_f = upload_to_gemini(target_file, mime_type)
        
        if not media_f:
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–ª–∏–µ–Ω—Ç—É –ø–æ–¥—Ä–æ–±–Ω—É—é –æ—à–∏–±–∫—É (–æ–Ω–∞ –±—É–¥–µ—Ç –≤ –∫–æ–Ω—Å–æ–ª–∏ –≤–æ—Ä–∫–µ—Ä–∞)
            return {"error": "Upload failed. Check Worker Logs for details."}
            
        files_cleanup.append(media_f)

        # 4. RAG
        db = SessionLocal()
        
        policies_text, human_examples = get_rag_context(db, profile, f"{filename} {shazam_text}", api_key)
        
        # –ú—ã –±–µ—Ä–µ–º —Ç–∞–∫—Å–æ–Ω–æ–º–∏—é –Ω–∞–ø—Ä—è–º—É—é –∏–∑ –±–∞–∑—ã, —Ç–∞–∫ –∫–∞–∫ –æ–Ω–∞ —Å—Ç–∞—Ç–∏—á–Ω–∞
        taxonomy_res = db.execute(text("SELECT code, title FROM taxonomy_label")).fetchall()
        taxonomy_text = "\n".join([f"- {t.code}: {t.title}" for t in taxonomy_res])

        # –°–æ–±–∏—Ä–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç —á–µ—Ä–µ–∑ .replace (—á—Ç–æ–±—ã –Ω–µ —Å–ª–æ–º–∞—Ç—å JSON-—Å–∫–æ–±–∫–∏)
        prompt = SYSTEM_PROMPT_TEMPLATE.replace("{policies_text}", policies_text)
        prompt = prompt.replace("{taxonomy_text}", taxonomy_text)
        prompt = prompt.replace("{human_examples}", human_examples)
        
        visual_instruction = f"–ü–†–û–§–ò–õ–¨ –ü–†–û–í–ï–†–ö–ò: {profile.upper()}. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–π –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç—Ä–æ–≥–æ –ø–æ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–º –ø–æ–ª–∏—Ç–∏–∫–∞–º. –í–ê–ñ–ù–û: –ê–Ω–∞–ª–∏–∑–∏—Ä—É–π –í–ò–î–ï–û–†–Ø–î. –û–±—Ä–∞—â–∞–π –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ –º–∏–º–∏–∫—É, –∂–µ—Å—Ç—ã –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–æ–∏—Å—Ö–æ–¥—è—â–µ–≥–æ (–∫–æ–º–µ–¥–∏—è, —Å—Å–æ—Ä–∞, –∏–≥—Ä–∞)."

        content = [prompt, visual_instruction, f"–§–∞–π–ª: {filename}. {shazam_text}", media_f]
        content = [x for x in content if x is not None]

        # 5. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
        self.update_state(state='PROGRESS', meta={'status': 'AI –¥—É–º–∞–µ—Ç...'})
        model = genai.GenerativeModel(MODEL_NAME)
        
        response = None
        max_retries = 5
        base_wait = 15
        
        for attempt in range(max_retries):
            try:
                response = model.generate_content(
                    content,
                    generation_config={"response_mime_type": "application/json"},
                    safety_settings=SAFETY_SETTINGS
                )
                break
            except Exception as e:
                if "429" in str(e) or "Quota" in str(e):
                    wait = base_wait * (2 ** attempt) + random.uniform(1, 5)
                    self.update_state(state='PROGRESS', meta={'status': f'–õ–∏–º–∏—Ç Google. –ñ–¥–µ–º {int(wait)}—Å...'})
                    time.sleep(wait)
                else: raise e

        if not response or not response.text: return {"error": "Empty response."}

        # 6. –§–∏–Ω–∏—à
        result_data = json.loads(clean_json_text(response.text))
        
        init_db()
        asset_res = db.execute(text("INSERT INTO media_asset (filename, duration_ms) VALUES (:fn, 0) RETURNING id"), {"fn": filename}).fetchone()
        asset_id = asset_res.id
        
        save_results_to_db(db, asset_id, result_data, MODEL_NAME)
        db.close()
        
        result_data['_asset_id'] = str(asset_id)
        result_data['_retrieved_context'] = human_examples 

        return result_data

    except Exception as e:
        print(f"CRITICAL: {e}")
        return {"error": str(e)}
    finally:
        for f in files_cleanup: 
            try: f.delete() 
            except: pass
        if compressed_path and os.path.exists(compressed_path): 
            os.remove(compressed_path)